---
title: "Data Science and Business Intel Project"
Subtitle: 'Churn Prediction: Telcom Customer Churn'
documentclass: article
output:
  html_document: default
  pdf_document: default
fontsize: 11pt
Author: Lok Tin Kevin Chan
---
```{r library, warning = FALSE, message= FALSE}
library(tidyverse)
library(ggplot2)
library(ggROC)
library(cowplot)
library(reshape2)
library(car)
library(caret)
library(leaps)
library(bestglm)
library(plotly)
library(DataExplorer)
library(purrr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)
library(pROC)
source("./[5]Script/Confusion_matrix.R")
source("./[5]Script/cutoff.R")
```

```{r data, warning=FALSE, message=FALSE}
# Read in raw data
ds <- (read.csv("./[4]source/WA_Fn-UseC_-Telco-Customer-Churn.csv"))
ds$SeniorCitizen <- as.factor(ds$SeniorCitizen)
#####################################################################################
# Metadata
#####################################################################################
# copy & Paste from datasource when free lol
#
#
#####################################################################################
# Data Type:
#####################################################################################
# 16 Categorical Variables:
# - 6 Binary Variables (Gender, Senior Citizen, Partner, Dependents, Phone Service, Paperless Billing)
# - 9 3-Factor level Variable (Multiple Lines, Internet Service, Online Security, Online Backup, Device Protection, Tech Support, Streaming TV, Streaming Movies, Contract)
# - 1 4-Factor level Variable (Payment Method)
#####################################################################################
# 3 Continious Variables:
# - Tenure, Monthly Charge, Total Charge
#####################################################################################
# 1 Target Variables:
# - Churn
#####################################################################################
```
## Data Cleaning
```{r}
# missing data analysis
# 1) We first check if missing exist within our dataset
print(paste0("The dataset contains missing data: ", any(is.na(ds))))
if (any(is.na(ds)) == "TRUE"){
      print(paste0("The total number of missing data(s) are: ", sum(is.na(ds))))
      print(paste0("The variable(s) with missing data(s) are: ", colnames(ds)[colSums(is.na(ds))>0]))
}
plot_missing(ds)

# 2) Filter the missing data into a its own dataset for further analysis
df_na <- ds[rowSums(is.na(ds))>0,]
df_na
```
From the above missing data analysis, we are able to see out of the 7043 observation of 21 variables there are only 11 missing values and they are belong to the TOTAL CHARGES column(.16%), hence we are working with a pretty clean dataset. 

An possible explaination for this mssing values is:
(1) These customer never paid anything to the company
(2) Tenure for all these customer are 0, thus meaning that this may be their first month with the company and thus the company hasn't charged them.

For these 11 missing data, we can either:
(1) Impute the total charge value
(2) Set total charge value to be zero
(3) Remove them from the data set

Since we have a relatively large dataset, and that none of the customer with missing value have churn, thus for convience of the analysis, we will drop the 11 observation with missing TOTAL CHARGE. 
## Data Exploration
```{r}
df_clean <- ds %>% 
            na.omit() %>% 
            select(-1)


# Binary variable distribution in Customer attribution
ggplot(data = df_clean, aes(x = Churn, y = (..count..)/sum(..count..), fill = Churn))+
      geom_bar()+
      ggtitle("Customer Attribution")+
      ylab("Percentage")

```

```{r}
# Categorical Variable Analysis
# Binary binary variables Analysis
options(repr.plot.width = 12, repr.plot.height = 8)
plot_grid(
  ggplot(data = df_clean, aes(gender, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(SeniorCitizen, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(Partner, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(Dependents, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(PhoneService, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(PaperlessBilling, fill = Churn))+geom_bar(position = "fill")
)

plot_grid(
  ggplot(data = df_clean, aes(MultipleLines, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(InternetService, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(OnlineBackup, fill = Churn))+geom_bar(position = "fill"),
   ggplot(data = df_clean, aes(OnlineSecurity, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(DeviceProtection, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(TechSupport, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(StreamingTV, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(StreamingMovies, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(Contract, fill = Churn))+geom_bar(position = "fill")
)

ggplot(data = df_clean, aes(x=PaymentMethod, fill=Churn))+
  geom_bar(position = "fill")


```
placeholder for the observation for binary variable analysis
- Senior Citizen
- Partner
- Dependent
- Paperless Billing

```{r}
# Categorical Analysis # Part 2


```

```{r}
# Continous Variable Analysis
p <- plot_ly(df_clean,
             x = ~MonthlyCharges,
             y = ~TotalCharges,
             z = ~tenure,
             color = ~Churn,
             marker = list(
               size = 2)) %>% 
             add_markers() %>% 
             layout(scene = list(
               xaxis = list(title = "Monthly Charges"),
               yaxis = list(title = "Total Charges"),
               zaxis = list(title = "Tenure")
             ))
p
```

```{r}
# Correlation matrix of continous variable analysis (thank you very much)
```

Placeholder for observation for the continous 


```{r}
# Data Cleaning and Standardization
df_clean2 <- df_clean %>% 
             mutate(MultipleLines=replace(MultipleLines,MultipleLines=="No phone service", "No")) %>%
             mutate(OnlineSecurity=replace(OnlineSecurity,OnlineSecurity=="No internet service","No")) %>% 
             mutate(DeviceProtection=replace(DeviceProtection,DeviceProtection=="No internet service","No")) %>% 
             mutate(TechSupport=replace(TechSupport,TechSupport=="No internet service","No")) %>% 
             mutate(StreamingTV=replace(StreamingTV,StreamingTV=="No internet service","No")) %>%             
             mutate(StreamingMovies=replace(StreamingMovies,StreamingMovies=="No internet service","No")) %>%
             mutate(OnlineBackup=replace(OnlineBackup,OnlineBackup=="No internet service","No")) %>% 
             mutate(tenure=scale(tenure)) %>% 
             mutate(MonthlyCharges=scale(MonthlyCharges)) %>% 
             mutate(TotalCharges=scale(TotalCharges))

```


```{r}
# Feature Importance Analysis
# generateFilterValuesData(task, "randomForest.importance")
df_clean2.feature <- randomForest(Churn~., data = df_clean2, importance = FALSE, ntree = 500, mtry = 2, do.trace=FALSE)

varImpPlot(df_clean2.feature)
```

```{r Split_Training}
# Split data into training and validation split
set.seed(1994)
training <- sample(2,nrow(df_clean2),replace=TRUE,prob=c(.8,.2))

```


```{r LogitRegression}
# GLM Analysis
# Still need to look at threshold analysis
# Full GLM
df_clean.fulllogit <- glm(Churn~., 
                          family = binomial,
                          data = df_clean2[training==1,])

getinfo(df_clean.fulllogit,df_clean2)[c("confusion_matrix", "accuracy","sensitivity")]

# Using Forward Approach to search for GLM model with lowest BIC 

# tmp.modelsearch <- bestglm(df_clean2[training==1,],IC = "BIC", family = binomial, method = "forward") # Takes a long while (>= 4 to 6 hours)
tmp.modelsearch$BestModels
tmp.modelsearch$BestModel

# Best GLM Model
df_clean.bestlogit <- glm(Churn~ 
                            SeniorCitizen + 
                            tenure + 
                            PhoneService + 
                            InternetService + 
                            OnlineSecurity + 
                            Contract + 
                            PaperlessBilling + 
                            PaymentMethod + 
                            TotalCharges, 
                          family = binomial,
                          data = df_clean2[training==1,])

summary(df_clean.bestlogit)
vif(df_clean.bestlogit)
getinfo(df_clean.bestlogit,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Remove Total Charges due to high VIF value (>2, thus multi-colinearity effect)

df_clean.bestlogit2 <- glm(Churn~ 
                            SeniorCitizen + 
                            tenure + 
                            PhoneService + 
                            InternetService + 
                            OnlineSecurity + 
                            Contract + 
                            PaperlessBilling +
                            PaymentMethod, 
                          family = binomial,
                          data = df_clean2[training==1,])
summary(df_clean.bestlogit2)
vif(df_clean.bestlogit2)
getinfo(df_clean.bestlogit2,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

```

```{r DecisionTree, warning=FALSE}
# Decision Tree Analysis
df_clean.fulltree <- rpart(Churn ~.,  
                       data = df_clean2[training==1,], method = "class",
                       control = rpart.control(cp=0)) 

getinfo(df_clean.fulltree,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Hyperparameter Tuning

# plotcp(df_clean.fulltree)
printcp(df_clean.fulltree)
tmp <- df_clean.fulltree$cptable[which.min(df_clean.fulltree$cptable[,"xerror"]),]

# Prune the tree
df_clean.besttree <- prune(df_clean.fulltree,cp = tmp[1])
rpart.plot(df_clean.besttree)
getinfo(df_clean.besttree,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]
```

```{r RandomForest, warning=FALSE}
# Random Forest
set.seed(1994)
df_clean.rforest <- randomForest(Churn~.,
                                 data = df_clean2[training==1,],
                                 ntree=500,                     # dataset
                                 cutoff=c(0.5,0.5), 
                                 mtry=2,
                                 importance=TRUE) 
df_clean.rforest

# Confusion Matrix Test
getinfo(df_clean.rforest,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Hyperparameter Tuning
set.seed(1994)
rforest.tune <- tuneRF(x = df_clean2[training==1,]%>%select(-Churn),
                       y = df_clean2[training==1,]$Churn,mtryStart=2,
                       ntreeTry = 500)

```

```{r SVM, warning=FALSE}
# SVM
df_clean.svm <- svm(Churn~.,
                    data = df_clean2[training==1,],
                    kernel = "linear",
                    cost = 0.01,
                    proability = TRUE)

getinfo(df_clean.svm,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Hyperparameter Tuning
svm.tune <- tune(svm,
                 Churn~.,
                 data = df_clean2[training==1,],
                 kernel = "linear",
                 ranges = list(cost = 10^(-5:0)))
                 
print(svm.tune)
svm.tune$best.model

df_clean.bestsvm <- svm(Churn~.,
                        data = df_clean2[training==1,],
                        kernel = "linear",
                        cost = 0.1,
                        probaility = TRUE)

getinfo(df_clean.bestsvm,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

```

```{r}
# Performance evaluation - Learning Curves and Fitted Graphs
# AUC Curve
# First assemble the probability matrix
prob_matrix <- data.frame(
               "logit" = predict(df_clean.bestlogit2,df_clean2[training==2,],type = "response"),
               "d_tree" = predict(df_clean.besttree, df_clean2[training==2,],type="prob")[,2],
               "r_forest" = predict(df_clean.rforest, df_clean2[training==2,], type = "prob")[,2],
               "svm" = as.numeric(attr(predict(df_clean.bestsvm, df_clean2[training==2,], decision.values = TRUE),"decision.values"))
               )
# Create the ROC Varible

logit.roc <- roc(df_clean2$Churn[training==2],prob_matrix$logit)
d_tree.roc <- roc(df_clean2$Churn[training==2],prob_matrix$d_tree)
r_forest.roc <- roc(df_clean2$Churn[training==2],prob_matrix$r_forest)
svm.roc <- roc(df_clean2$Churn[training==2],prob_matrix$svm)

ggroc(list(logit=logit.roc,decision.tree=d_tree.roc,random.forest=r_forest.roc,svm=svm.roc),legacy.axes = FALSE)

tmp4 <- c(logit.roc$auc,d_tree.roc$auc,r_forest.roc$auc,svm.roc$auc)

tmp5 <- data.frame(
  "AUC" = tmp4
)
row.names(tmp5)<- c("Logit","Decision Tree", "Random Forest","SVM")

tmp5
```
A small discussion about cutoff point:

as we are attempting to identify customer that are going to churn, we thus need to focus on sensitivity metric compared to accuracy. As it is comparitively more expensive to acquire customer than retain customer, thus we are not as concern with false positive, but rather concerned with false negative. We would idealy like a model that is able to successful target all customer that are going to churn, and it should matter less if we have a higher number of false postive to us a telcommunication company. Thus we should have a lower threshold value than 0.5, though the actual value often require domain knowledge which we lack, thus we are going to use a more objective method to set out threshold value. 
```{r}
# Lets use logistic regression as it has the largest AUC out of all three method

output <- matrix(0,100,3)
x_axis <- seq(0.01,0.8,length=100)

for (i in 1:100)
{
  output[i,]=threshold(x_axis[i])
}

plot(x_axis,output[,1], type = "l", col = "darkgreen", xlab = "Threshold Value", ylab = "Values")
lines(x_axis,output[,2],col = "red")
lines(x_axis,output[,3], col = "blue")
legend("bottom",col=c(2,"darkgreen",4,"darkred"),text.font =3,inset = 0.02,
       box.lty=0,cex = 0.8, 
       lwd=c(2,2,2,2),c("Specificity","Senitivity","Accuracy"))


x_axis[which(abs(output[,1]-output[,2])<0.01)]
```


