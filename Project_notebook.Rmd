---
title: "Data Science and Business Intel Project"
Subtitle: 'Churn Prediction: Telcom Customer Churn'
documentclass: article
output:
  html_document: default
  pdf_document: default
fontsize: 11pt
Author: Lok Tin Kevin Chan
---
```{r library, warning = FALSE, message= FALSE}
library(tidyverse)
library(ggplot2)
library(ggROC)
library(cowplot)
library(reshape2)
library(car)
library(leaps)
library(bestglm)
library(plotly)
library(DataExplorer)
library(purrr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(e1071)
library(pROC)
source("./[5]Script/Confusion_matrix.R")
```

```{r data, warning=FALSE, message=FALSE}
# Read in raw data
ds <- (read.csv("./[4]source/WA_Fn-UseC_-Telco-Customer-Churn.csv"))
ds$SeniorCitizen <- as.factor(ds$SeniorCitizen)
#####################################################################################
# Metadata
#####################################################################################
# copy & Paste from datasource when free lol
#
#
#####################################################################################
# Data Type:
#####################################################################################
# 16 Categorical Variables:
# - 6 Binary Variables (Gender, Senior Citizen, Partner, Dependents, Phone Service, Paperless Billing)
# - 9 3-Factor level Variable (Multiple Lines, Internet Service, Online Security, Online Backup, Device Protection, Tech Support, Streaming TV, Streaming Movies, Contract)
# - 1 4-Factor level Variable (Payment Method)
#####################################################################################
# 3 Continious Variables:
# - Tenure, Monthly Charge, Total Charge
#####################################################################################
# 1 Target Variables:
# - Churn
#####################################################################################
```
## Data Cleaning
```{r}
# missing data analysis
# 1) We first check if missing exist within our dataset
print(paste0("The dataset contains missing data: ", any(is.na(ds))))
if (any(is.na(ds)) == "TRUE"){
      print(paste0("The total number of missing data(s) are: ", sum(is.na(ds))))
      print(paste0("The variable(s) with missing data(s) are: ", colnames(ds)[colSums(is.na(ds))>0]))
}
plot_missing(ds)

# 2) Filter the missing data into a its own dataset for further analysis
df_na <- ds[rowSums(is.na(ds))>0,]
df_na
```
From the above missing data analysis, we are able to see out of the 7043 observation of 21 variables there are only 11 missing values and they are belong to the TOTAL CHARGES column(.16%), hence we are working with a pretty clean dataset. 

An possible explaination for this mssing values is:
(1) These customer never paid anything to the company
(2) Tenure for all these customer are 0, thus meaning that this may be their first month with the company and thus the company hasn't charged them.

For these 11 missing data, we can either:
(1) Impute the total charge value
(2) Set total charge value to be zero
(3) Remove them from the data set

Since we have a relatively large dataset, and that none of the customer with missing value have churn, thus for convience of the analysis, we will drop the 11 observation with missing TOTAL CHARGE. 
## Data Exploration
```{r}
df_clean <- ds %>% 
            na.omit() %>% 
            select(-1)


# Binary variable distribution in Customer attribution
ggplot(data = df_clean, aes(x = Churn, y = (..count..)/sum(..count..), fill = Churn))+
      geom_bar()+
      ggtitle("Customer Attribution")+
      ylab("Percentage")

```

```{r}
# Categorical Variable Analysis
# Binary binary variables Analysis
options(repr.plot.width = 12, repr.plot.height = 8)
plot_grid(
  ggplot(data = df_clean, aes(gender, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(SeniorCitizen, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(Partner, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(Dependents, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(PhoneService, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(PaperlessBilling, fill = Churn))+geom_bar(position = "fill")
)

plot_grid(
  ggplot(data = df_clean, aes(MultipleLines, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(InternetService, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(OnlineBackup, fill = Churn))+geom_bar(position = "fill"),
   ggplot(data = df_clean, aes(OnlineSecurity, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(DeviceProtection, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(TechSupport, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(StreamingTV, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(StreamingMovies, fill = Churn))+geom_bar(position = "fill"),
  ggplot(data = df_clean, aes(Contract, fill = Churn))+geom_bar(position = "fill")
)

ggplot(data = df_clean, aes(x=PaymentMethod, fill=Churn))+
  geom_bar(position = "fill")


```
placeholder for the observation for binary variable analysis
- Senior Citizen
- Partner
- Dependent
- Paperless Billing

```{r}
# Categorical Analysis # Part 2


```

```{r}
# Continous Variable Analysis
p <- plot_ly(df_clean,
             x = ~MonthlyCharges,
             y = ~TotalCharges,
             z = ~tenure,
             color = ~Churn,
             marker = list(
               size = 2)) %>% 
             add_markers() %>% 
             layout(scene = list(
               xaxis = list(title = "Monthly Charges"),
               yaxis = list(title = "Total Charges"),
               zaxis = list(title = "Tenure")
             ))
p
```
Placeholder for observation for the continous 

```{r}
# Data Cleaning and Standardization
df_clean2 <- df_clean %>% 
             mutate(MultipleLines=replace(MultipleLines,MultipleLines=="No phone service", "No")) %>%
             mutate(OnlineSecurity=replace(OnlineSecurity,OnlineSecurity=="No internet service","No")) %>% 
             mutate(DeviceProtection=replace(DeviceProtection,DeviceProtection=="No internet service","No")) %>% 
             mutate(TechSupport=replace(TechSupport,TechSupport=="No internet service","No")) %>% 
             mutate(StreamingTV=replace(StreamingTV,StreamingTV=="No internet service","No")) %>%             
             mutate(StreamingMovies=replace(StreamingMovies,StreamingMovies=="No internet service","No")) %>%
             mutate(OnlineBackup=replace(OnlineBackup,OnlineBackup=="No internet service","No")) %>% 
             mutate(tenure=scale(tenure)) %>% 
             mutate(MonthlyCharges=scale(MonthlyCharges)) %>% 
             mutate(TotalCharges=scale(TotalCharges))

```


```{r}
# Feature Importance Analysis
# generateFilterValuesData(task, "randomForest.importance")
df_clean2.feature <- randomForest(Churn~., data = df_clean2, importance = FALSE, ntree = 500, mtry = 2, do.trace=FALSE)

varImpPlot(df_clean2.feature)
```

```{r Split_Training}
# Split data into training and validation split
set.seed(1994)
training <- sample(2,nrow(df_clean2),replace=TRUE,prob=c(.8,.2))

```


```{r LogitRegression}
# GLM Analysis
# Still need to look at threshold analysis
# Full GLM
df_clean.fulllogit <- glm(Churn~., 
                          family = binomial,
                          data = df_clean2[training==1,])

getinfo(df_clean.fulllogit,df_clean2)[c("confusion_matrix", "accuracy","sensitivity")]

# Using Forward Approach to search for GLM model with lowest BIC 

# tmp.modelsearch <- bestglm(df_clean2[training==1,],IC = "BIC", family = binomial, method = "forward") # Takes a long while (>= 4 to 6 hours)
tmp.modelsearch$BestModels
tmp.modelsearch$BestModel

# Best GLM Model
df_clean.bestlogit <- glm(Churn~ 
                            SeniorCitizen + 
                            tenure + 
                            PhoneService + 
                            InternetService + 
                            OnlineSecurity + 
                            Contract + 
                            PaperlessBilling + 
                            PaymentMethod + 
                            TotalCharges, 
                          family = binomial,
                          data = df_clean2[training==1,])

summary(df_clean.bestlogit)
vif(df_clean.bestlogit)
getinfo(df_clean.bestlogit,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Remove Total Charges due to high VIF value (>2, thus multi-colinearity effect) and also Payment method because of the high P-Value(not signifi cant)

df_clean.bestlogit2 <- glm(Churn~ 
                            SeniorCitizen + 
                            tenure + 
                            PhoneService + 
                            InternetService + 
                            OnlineSecurity + 
                            Contract + 
                            PaperlessBilling +
                            PaymentMethod, 
                          family = binomial,
                          data = df_clean2[training==1,])
summary(df_clean.bestlogit2)
vif(df_clean.bestlogit2)
getinfo(df_clean.bestlogit2,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

```

```{r DecisionTree, warning=FALSE}
# Decision Tree Analysis
df_clean.fulltree <- rpart(Churn ~.,  
                       data = df_clean2[training==1,], method = "class",
                       control = rpart.control(cp=0)) 

getinfo(df_clean.fulltree,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Hyperparameter Tuning

# plotcp(df_clean.fulltree)
printcp(df_clean.fulltree)
tmp <- df_clean.fulltree$cptable[which.min(df_clean.fulltree$cptable[,"xerror"]),]

# Prune the tree
df_clean.besttree <- prune(df_clean.fulltree,cp = tmp[1])
rpart.plot(df_clean.besttree)
getinfo(df_clean.besttree,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]
```

```{r RandomForest, warning=FALSE}
# Random Forest
set.seed(1994)
df_clean.rforest <- randomForest(Churn~.,
                                 data = df_clean2[training==1,],
                                 ntree=500,                     # dataset
                                 cutoff=c(0.5,0.5), 
                                 mtry=2,
                                 importance=TRUE) 
df_clean.rforest

# Confusion Matrix Test
getinfo(df_clean.rforest,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Hyperparameter Tuning
set.seed(1994)
rforest.tune <- tuneRF(x = df_clean2[training==1,]%>%select(-Churn),
                       y = df_clean2[training==1,]$Churn,mtryStart=2,
                       ntreeTry = 500)

```

```{r SVM, warning=FALSE}
# SVM
df_clean.svm <- svm(Churn~.,
                    data = df_clean2[training==1,],
                    kernel = "linear",
                    cost = 0.01,
                    proability = TRUE)

getinfo(df_clean.svm,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

# Hyperparameter Tuning
svm.tune <- tune(svm,
                 Churn~.,
                 data = df_clean2[training==1,],
                 kernel = "linear",
                 ranges = list(cost = 10^(-5:0)))
                 
print(svm.tune)
svm.tune$best.model

df_clean.bestsvm <- svm(Churn~.,
                        data = df_clean2[training==1,],
                        kernel = "linear",
                        cost = 0.1,
                        probaility = TRUE)

getinfo(df_clean.bestsvm,df_clean2)[c("confusion_matrix", "accuracy", "sensitivity")]

```

```{r}
# Performance evaluation - Learning Curves and Fitted Graphs
# AUC Curve
# First assemble the probability matrix
prob_matrix <- data.frame(
               "logit" = predict(df_clean.bestlogit2,df_clean2[training==2,],type = "response"),
               "d_tree" = predict(df_clean.besttree, df_clean2[training==2,],type="prob")[,2],
               "r_forest" = predict(df_clean.rforest, df_clean2[training==2,], type = "prob")[,2],
               "svm" = as.numeric(attr(predict(df_clean.bestsvm, df_clean2[training==2,], decision.values = TRUE),"decision.values"))
               )
# Create the ROC Varible

logit.roc <- roc(df_clean2$Churn[training==2],prob_matrix$logit)
d_tree.roc <- roc(df_clean2$Churn[training==2],prob_matrix$d_tree)
r_forest.roc <- roc(df_clean2$Churn[training==2],prob_matrix$r_forest)
svm.roc <- roc(df_clean2$Churn[training==2],prob_matrix$svm)

plot(logit.roc,legacy.axes = TRUE, print.auc.y = 1.0, print.auc = TRUE)
plot(d_tree.roc,legacy.axes = TRUE, print.auc.y = 1.0, print.auc = TRUE)
plot(r_forest.roc,legacy.axes = TRUE, print.auc.y = 1.0, print.auc = TRUE)
plot(svm.roc,legacy.axes = TRUE, print.auc.y = 1.0, print.auc = TRUE)

ggroc(list(logit=logit.roc,d_tree=d_tree.roc,r_forest=r_forest.roc,svm=svm.roc),legacy.axes = FALSE)





```


